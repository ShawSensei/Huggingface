{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-14 00:44:27.025228: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-14 00:44:27.025272: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-14 00:44:27.077868: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-14 00:44:28.283517: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.9985001087188721}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "res = classifier(\"I get unreasonably angry with John sometimes. I'm sure I never used to be so sensitive. I think it is due to this nervous condition. But John says if I feel so, I shall neglect proper self-control; so I take pains to control myself-before him, at least, and that makes me very tired.\")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': 'This is a course about Python list comprehension', 'labels': ['education', 'business', 'politics'], 'scores': [0.9622026085853577, 0.02684136852622032, 0.010956005193293095]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"zero-shot-classification\")\n",
    "\n",
    "res = generator(\n",
    "    \"This is a course about Python list comprehension\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
    ")\n",
    "    \n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.9985001087188721}]\n",
      "[{'label': 'NEGATIVE', 'score': 0.9985001087188721}]\n",
      "{'input_ids': [101, 2478, 1037, 10938, 2121, 2897, 2003, 3722, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['using', 'a', 'transform', '##er', 'network', 'is', 'simple']\n",
      "[2478, 1037, 10938, 2121, 2897, 2003, 3722]\n",
      "using a transformer network is simple\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "res = classifier(\"I get unreasonably angry with John sometimes. I'm sure I never used to be so sensitive. I think it is due to this nervous condition. But John says if I feel so, I shall neglect proper self-control; so I take pains to control myself-before him, at least, and that makes me very tired.\")\n",
    "\n",
    "print(res)\n",
    "model_name=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\",model=model, tokenizer = tokenizer)\n",
    "res = classifier(\"I get unreasonably angry with John sometimes. I'm sure I never used to be so sensitive. I think it is due to this nervous condition. But John says if I feel so, I shall neglect proper self-control; so I take pains to control myself-before him, at least, and that makes me very tired.\")\n",
    "print(res)\n",
    "sequence = \"Using a Transformer network is simple\"\n",
    "res = tokenizer(sequence)\n",
    "print (res)\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "print(tokens)\n",
    "ids= tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)\n",
    "decoded_string = tokenizer.decode(ids)\n",
    "print(decoded_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9598048329353333}, {'label': 'POSITIVE', 'score': 0.9998252987861633}]\n",
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101, 18750,  2003,  2307,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.5607,  1.6123],\n",
      "        [-4.1691,  4.4828]]), hidden_states=None, attentions=None)\n",
      "tensor([[4.0195e-02, 9.5980e-01],\n",
      "        [1.7478e-04, 9.9983e-01]])\n",
      "tensor([1, 1])\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "model_name=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "X_train=[\"I've been waiting for a HuggingFace course my whole life.\",\"Python is great\"]\n",
    "\n",
    "res = classifier(X_train)\n",
    "print(res)\n",
    "\n",
    "batch = tokenizer(X_train, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "print(batch)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**batch)\n",
    "    print(outputs)\n",
    "    predictions = F.softmax(outputs.logits, dim=1)\n",
    "    print(predictions)\n",
    "    labels = torch.argmax(predictions, dim=1)\n",
    "    print(labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998785257339478},\n",
       " {'label': 'POSITIVE', 'score': 0.9680057168006897},\n",
       " {'label': 'NEGATIVE', 'score': 0.8776116371154785}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(task=\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "text_list=[\"This is great\",\\\n",
    "           \"Thanks for nothing\",\\\n",
    "           \"You've got to work on your face\",\\\n",
    "\n",
    "]\n",
    "classifier(text_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'admiration', 'score': 0.9526104927062988},\n",
       "  {'label': 'approval', 'score': 0.030472073704004288},\n",
       "  {'label': 'neutral', 'score': 0.015236238949000835},\n",
       "  {'label': 'excitement', 'score': 0.006063767243176699},\n",
       "  {'label': 'gratitude', 'score': 0.005296194460242987},\n",
       "  {'label': 'joy', 'score': 0.004475215449929237},\n",
       "  {'label': 'curiosity', 'score': 0.004322330001741648},\n",
       "  {'label': 'realization', 'score': 0.004089601803570986},\n",
       "  {'label': 'optimism', 'score': 0.00407722033560276},\n",
       "  {'label': 'disapproval', 'score': 0.004076560027897358},\n",
       "  {'label': 'annoyance', 'score': 0.0035287411883473396},\n",
       "  {'label': 'surprise', 'score': 0.0029730673413723707},\n",
       "  {'label': 'disappointment', 'score': 0.002734638284891844},\n",
       "  {'label': 'love', 'score': 0.00269457814283669},\n",
       "  {'label': 'amusement', 'score': 0.0024867462925612926},\n",
       "  {'label': 'confusion', 'score': 0.0023607409093528986},\n",
       "  {'label': 'pride', 'score': 0.0021013382356613874},\n",
       "  {'label': 'sadness', 'score': 0.0017730530817061663},\n",
       "  {'label': 'anger', 'score': 0.0017196922563016415},\n",
       "  {'label': 'caring', 'score': 0.0013670086627826095},\n",
       "  {'label': 'desire', 'score': 0.0010478721233084798},\n",
       "  {'label': 'disgust', 'score': 0.0009689942235127091},\n",
       "  {'label': 'fear', 'score': 0.0005249778041616082},\n",
       "  {'label': 'relief', 'score': 0.0004862115893047303},\n",
       "  {'label': 'embarrassment', 'score': 0.00034175071050412953},\n",
       "  {'label': 'grief', 'score': 0.0003389191988389939},\n",
       "  {'label': 'remorse', 'score': 0.0002780948707368225},\n",
       "  {'label': 'nervousness', 'score': 0.00020788486290257424}]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(task=\"text-classification\",model=\"Samlowe/roberta-base-go_emotions\",top_k=None)\n",
    "classifier(text_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do Do Hee is the successor of the Future Group. She has an arrogant and cool-headed personality, who doesn't trust in anyone. She is cynical about love. She gets involved with a demon named Jung Koo Won.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "summerizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "text = \"\"\"Do Do Hee is the successor of the Future Group. She has an arrogant and cool-headed personality, who doesnâ€™t trust in anyone. She is cynical about love. Do Do Hee gets involved with a demon named Jung Koo Won and makes a contract marriage with him. She faces big changes in her life. Jung Koo Won is a demon. He can live for eternity by making dangerous, but sweet deals with humans who endure hellish lives. He looks down upon humans and he has prowled over this world like an apex predator for 200 years. He gets involved with Do Do Hee and somehow loses his power all of a sudden. He then enters into a contract marriage with her. To prevent his own extinction, he must protect Do Do Hee who has taken all of his power. Their relationship develops romantically.\"\"\"\n",
    "summarized_text = summerizer(text,min_length=5, max_length=140)[0]['summary_text']\n",
    "print(summarized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "No chat template is defined for this tokenizer - using the default template for the BlenderbotTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Conversation id: 9347577f-e450-4b7b-977e-ece4ea5bf196\n",
       "user: Hi I'm a quack, How are you?\n",
       "assistant:  I'm doing well, how about yourself? What do you like to do in your spare time?\n",
       "user: kiss me \n",
       "assistant:  I don't think I've kissed anyone in my life. Do you have any siblings?"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Conversation, pipeline\n",
    "\n",
    "chatbot = pipeline(\"conversational\", model=\"facebook/blenderbot-400M-distill\")\n",
    "conversation = Conversation(\"Hi I'm a quack, How are you?\")\n",
    "conversation = chatbot(conversation)\n",
    "conversation\n",
    "\n",
    "\n",
    "user=input(\"say bitch -\")\n",
    "conversation.add_user_input(user)\n",
    "conversation = chatbot(conversation)\n",
    "conversation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Conversation, pipeline\n",
    "import gradio as gr\n",
    "message_list =[]\n",
    "\n",
    "response_list =[]\n",
    "\n",
    "def vanilla_chatbot(message, history):\n",
    "    conversation = Conversation(text=message, past_user_inputs=message_list, generated_responses=response_list)\n",
    "    conversation = chatbot(conversation)\n",
    "\n",
    "    return conversation.generated_responses[-1]\n",
    "\n",
    "demo_chatbot = gr.ChatInterface(vanilla_chatbot, title=\"Vanilla Chatbot\", description=\"Enter text to start chatting\")\n",
    "\n",
    "demo_chatbot.launch()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "\n",
    "from transformers import(\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import PeftModel, PeftConfig, get_peft_model,LoraConfig\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#base model of bert = distilbert\n",
    "\n",
    "model_checkpoint = 'distilbert-base-uncased'\n",
    "\n",
    "#label maps\n",
    "id2label = {0:\"Negative\", 1:\"Positive\"}\n",
    "label2id = {\"Negative\":0, \"Positive\":1}\n",
    "\n",
    "#generate classification model\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint,num_labels=2, id2label=id2label,label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#load dataset\n",
    "dataset = load_dataset(\"shawhin/imdb-truncated\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['label', 'text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preprocess data\n",
    "#create tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)\n",
    "\n",
    "#create tokenize function(translate text to number)\n",
    "def tokenize_function(examples):\n",
    "    #extract text\n",
    "    text=examples[\"text\"]\n",
    "\n",
    "    #tokenize and truncate text (in order to make it same size or length)\n",
    "    tokenizer.truncation_side =\"left\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    return tokenized_inputs\n",
    "#add pad token if none exists\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token':'[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "#tokenize training and validation datasets\n",
    "tokenize_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenize_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_562/4279567013.py:8: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  accuracy = load_metric(\"accuracy\", trust_remote_code=True)\n"
     ]
    }
   ],
   "source": [
    "#create data collator (dynamically pad for given batch to match the longer one, computationally efficient )\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "#evaluation metrics for accuracy\n",
    "#import accuracy evaluation function to pass into trainer later\n",
    "from datasets import load_metric\n",
    "\n",
    "# Load accuracy metric with trust_remote_code=True\n",
    "accuracy = load_metric(\"accuracy\", trust_remote_code=True)\n",
    "\n",
    "# Define your compute_metrics function\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": accuracy.compute(predictions=predictions, references=labels)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained model predictions:\n",
      "----------------------------\n",
      "It was good. - Negative\n",
      "Not a fan, don't recommed. - Negative\n",
      "Better than the first one. - Negative\n",
      "This is not worth watching even once. - Negative\n",
      "This one is a pass. - Negative\n"
     ]
    }
   ],
   "source": [
    "# define list of examples\n",
    "text_list = [\"It was good.\", \"Not a fan, don't recommed.\", \n",
    "\"Better than the first one.\", \"This is not worth watching even once.\", \n",
    "\"This one is a pass.\"]\n",
    "\n",
    "print(\"Untrained model predictions:\")\n",
    "\n",
    "print(\"----------------------------\")\n",
    "\n",
    "for text in text_list:\n",
    "    #tokenize text\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    #compute logits \n",
    "    logits = model(inputs).logits\n",
    "    #then to label\n",
    "    predictions = torch.argmax(logits)\n",
    "\n",
    "    print(text + \" - \" + id2label[predictions.tolist()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 628,994 || all params: 67,584,004 || trainable%: 0.9306847223789819\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(task_type=\"SEQ_CLS\",\n",
    "                         r=4,\n",
    "                         lora_alpha=32,\n",
    "                         lora_dropout=0.01,\n",
    "                         target_modules = ['q_lin']\n",
    "#LoRA configuration\n",
    "\n",
    ") \n",
    "model = get_peft_model(model,peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Hyperparameters\n",
    "lr = 1e-3  # Size of optimization step\n",
    "batch_size = 4  # Number of examples processed per optimization step\n",
    "num_epochs = 10  # Number of times model runs through training data\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_checkpoint + \"-lora-text-classification\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 09:47, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.418964</td>\n",
       "      <td>{'accuracy': 0.885}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.410300</td>\n",
       "      <td>0.432730</td>\n",
       "      <td>{'accuracy': 0.883}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.410300</td>\n",
       "      <td>0.510350</td>\n",
       "      <td>{'accuracy': 0.888}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.181800</td>\n",
       "      <td>0.705402</td>\n",
       "      <td>{'accuracy': 0.89}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.181800</td>\n",
       "      <td>0.775070</td>\n",
       "      <td>{'accuracy': 0.882}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.068800</td>\n",
       "      <td>0.823610</td>\n",
       "      <td>{'accuracy': 0.889}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.068800</td>\n",
       "      <td>0.884313</td>\n",
       "      <td>{'accuracy': 0.888}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.018900</td>\n",
       "      <td>0.907857</td>\n",
       "      <td>{'accuracy': 0.891}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.018900</td>\n",
       "      <td>0.952116</td>\n",
       "      <td>{'accuracy': 0.886}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.007900</td>\n",
       "      <td>0.928465</td>\n",
       "      <td>{'accuracy': 0.892}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'accuracy': 0.885}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Checkpoint destination directory distilbert-base-uncased-lora-text-classification/checkpoint-250 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Trainer is attempting to log a value of \"{'accuracy': 0.883}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Checkpoint destination directory distilbert-base-uncased-lora-text-classification/checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Trainer is attempting to log a value of \"{'accuracy': 0.888}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Checkpoint destination directory distilbert-base-uncased-lora-text-classification/checkpoint-750 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Trainer is attempting to log a value of \"{'accuracy': 0.89}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Checkpoint destination directory distilbert-base-uncased-lora-text-classification/checkpoint-1000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Trainer is attempting to log a value of \"{'accuracy': 0.882}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Checkpoint destination directory distilbert-base-uncased-lora-text-classification/checkpoint-1250 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Trainer is attempting to log a value of \"{'accuracy': 0.889}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Checkpoint destination directory distilbert-base-uncased-lora-text-classification/checkpoint-1500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Trainer is attempting to log a value of \"{'accuracy': 0.888}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Checkpoint destination directory distilbert-base-uncased-lora-text-classification/checkpoint-1750 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Trainer is attempting to log a value of \"{'accuracy': 0.891}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Checkpoint destination directory distilbert-base-uncased-lora-text-classification/checkpoint-2000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Trainer is attempting to log a value of \"{'accuracy': 0.886}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Checkpoint destination directory distilbert-base-uncased-lora-text-classification/checkpoint-2250 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Trainer is attempting to log a value of \"{'accuracy': 0.892}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Checkpoint destination directory distilbert-base-uncased-lora-text-classification/checkpoint-2500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2500, training_loss=0.1375349624633789, metrics={'train_runtime': 588.481, 'train_samples_per_second': 16.993, 'train_steps_per_second': 4.248, 'total_flos': 1113026652407424.0, 'train_loss': 0.1375349624633789, 'epoch': 10.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creater trainer object\n",
    "trainer = Trainer(\n",
    "    model=model, # our peft model\n",
    "    args=training_args, # hyperparameters\n",
    "    train_dataset=tokenize_dataset[\"train\"], # training data\n",
    "    eval_dataset=tokenize_dataset[\"validation\"], # validation data\n",
    "    tokenizer=tokenizer, # define tokenizer\n",
    "    data_collator=data_collator, # this will dynamically pad examples in each batch to be equal length\n",
    "    compute_metrics=compute_metrics, # evaluates model using compute_metrics() function from before\n",
    ")\n",
    "\n",
    "# train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model predictions:\n",
      "--------------------------\n",
      "It was good. - Positive\n",
      "Not a fan, don't recommed. - Negative\n",
      "Better than the first one. - Positive\n",
      "This is not worth watching even once. - Negative\n",
      "This one is a pass. - Negative\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming you have already loaded the tokenizer, model, text_list, and id2label\n",
    "\n",
    "# Move model to 'cpu' or 'cuda' based on your requirements\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "print(\"Trained model predictions:\")\n",
    "print(\"--------------------------\")\n",
    "for text in text_list:\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    logits = model(inputs).logits\n",
    "    predictions = torch.max(logits, 1).indices\n",
    "\n",
    "    print(text + \" - \" + id2label[predictions.tolist()[0]])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
